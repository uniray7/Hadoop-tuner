import os
import math
import json
import commands
import ParseLog

Heap1 = "-Xmx"
Heap2 = "M"

def extract_configs():
	config_file = open('config.json','r')
	config_json = json.load(config_file)
	configs = config_json['sample_standard_list']
	return configs

def modify_configs(configs,values):
	cmd = XML_PARSER_DIR+'/revise_conf_v1 '+CONF_DIR+'/conf_new/mapred-site.xml'
	i=0
	for conf in configs:
		if conf["type"]=="heap":
			cmd = cmd+str(configs[i]["name"])+Heap1+str(values[i])+Heap2
		elif conf["type"]=="split":
			cmd = cmd+str(configs[i]["name"])+str(values[i]*(2**20))
		else:
			cmd = cmd+str(configs[i]["name"])+str(values[i])
		i=i+1
	os.system(cmd)


def copy_conf_dir():
	cmd = 'cp -r '+CONF_DIR+'/conf_default '+CONF_DIR+'/conf_new'
	os.system(cmd)

def rm_conf_dir():
	cmd = 'rm -rf '+CONF_DIR+'/conf_new'
	os.system(cmd)

def rm_hdfs_output(output_dir):
	cmd = 'hadoop fs -rmr '+output_dir
	os.system(cmd)


def get_hdfs_log(output_dir,serial_num):
	cmd = 'hadoop fs -get '+output_dir+'/_logs '+LOG_DIR+'/log_'+serial_num
	os.system(cmd)	


def gen_log_serial_num():
	cmd = 'ls '+LOG_DIR+'| wc -l'
	serial_num = commands.getoutput(cmd)
	return serial_num

def get_inputdata_size(input_dir):
	cmd = "hadoop fs -dus "+input_dir
	result = commands.getoutput(cmd)
	result = result.split('\t')
	DataSize = int(result[1])/(2**20)
	return DataSize





def run_job(input_dir, output_dir, config_values):
	
	copy_conf_dir()
	
	modify_configs(configs,config_values)


	cmd = 'hadoop --config '+CONF_DIR+'/conf_new jar '+HADOOP_HOME+'/hadoop-examples* terasort '+input_dir+' '+output_dir
	os.system(cmd)


	log_serial_num = gen_log_serial_num()
	get_hdfs_log(output_dir,log_serial_num)				

	rm_conf_dir()
	rm_hdfs_output(output_dir)
	
	log_path = LOG_DIR+'/log_'+log_serial_num
	return log_path





#extract tuned configs
configs = extract_configs()

#setup environment and path
PATHs_file = open('environment.json','r')
PATHs = json.load(PATHs_file)

HADOOP_HOME = PATHs['HADOOP_HOME']
CONF_DIR = PATHs['CONF_DIR']
XML_PARSER_DIR = PATHs['XML_PARSER_DIR']
LOG_DIR = PATHs['LOG_DIR']


#Whether the model of the application is existed or not
#If yes, invoke Optimizer to get recommended config. 
#If not, run the job with random config

#if Is_model_exist == true:
#	optimal_config_paras = Optimizer()
#	run_job(input_dir,output_dir,optimal_config_paras)
#else:
#	random_config_paras = random_gen(configs)
#	run_job(input_dir,output_dir,random_config_paras)

###augements#############
input_dir = '/tera-in'
output_dir = '/tera-out'
#########################


ran_file = open('random_file','r')
num=0
for para_str in ran_file:
	data_list = json.loads(para_str)
	
	#run a job, after the job finishing, get log directory path
	#start to deal with the log
	#log_path=run_job(input_dir,output_dir,data_list)
	log_path = LOG_DIR+'/log_0'	
	inputdata_size = get_inputdata_size(input_dir)

	job_log = ParseLog.Log(log_path,inputdata_size)
	job_log.Raw_to_RawJson()
	job_log.RefineJson_SaveIn_DB()


